# #!/usr/bin/env python3
# # -*- coding: utf-8 -*-

# """
# ç”Ÿæˆ episodes.jsonlï¼š
# - æ¯ä¸ªé•¿ç¨‹ä»»åŠ¡ p å¯¹åº”ä¸€ä¸ª episode
# - è¯»å–å¯¹åº” q ä¸‹çš„ instruction.txt ä½œä¸ºä»»åŠ¡
# - length = p/p-1 ä¸‹æ‰€æœ‰ data.csv çš„è¡Œæ•°ï¼ˆå»æ‰æ ‡é¢˜è¡Œï¼‰ç´¯åŠ 
# """

# from pathlib import Path
# import csv
# import json
# import os

# # # ========== é…ç½® ==========
# # REORG_ROOT = Path("/data2/konghanlin/internmanip/data/datasets/our_reorganized_data")
# # OUTPUT_FILE = r"/data2/konghanlin/internmanip/data/datasets/output_small/meta/episodes.jsonl"
# # # ========================

# import argparse

# # è§£æå‘½ä»¤è¡Œå‚æ•°
# parser = argparse.ArgumentParser(description='ç”Ÿæˆepisodes.jsonlæ–‡ä»¶çš„è„šæœ¬')
# parser.add_argument('--reorg_root', required=True, help='é‡ç»„æ•°æ®çš„æ ¹ç›®å½•è·¯å¾„')
# parser.add_argument('--output_file', required=True, help='ç”Ÿæˆçš„episodes.jsonlæ–‡ä»¶è·¯å¾„')
# args = parser.parse_args()

# # ========== é…ç½®ï¼ˆä»å‘½ä»¤è¡Œå‚æ•°è¯»å–ï¼‰ ==========
# REORG_ROOT = Path(args.reorg_root)
# OUTPUT_FILE = args.output_file
# # ========================

# episode_index = 0
# lines = []

# # éå† q
# for q_dir in sorted([p for p in REORG_ROOT.iterdir() if p.is_dir()], key=lambda x: int(x.name)):
#     instr_file = q_dir / "instruction.txt"
#     if not instr_file.exists():
#         print(f"[WARN] {instr_file} ä¸å­˜åœ¨ï¼Œè·³è¿‡")
#         continue
#     with open(instr_file, "r", encoding="utf-8") as f:
#         task_instr = f.read().strip()

#     # éå† p
#     for p_dir in sorted([p for p in q_dir.iterdir() if p.is_dir()], key=lambda x: int(x.name)):
#         # p-1 ä¸‹çš„ data.csv ç»Ÿè®¡è¡Œæ•°
#         total_length = 0
#         for short_task_dir in sorted([p for p in p_dir.iterdir() if p.is_dir()]):
#             data_csv = short_task_dir / "data.csv"
#             if not data_csv.exists():
#                 print(f"[WARN] {data_csv} ä¸å­˜åœ¨ï¼Œè·³è¿‡")
#                 continue
#             with open(data_csv, "r", encoding="utf-8") as f:
#                 reader = csv.reader(f)
#                 next(reader, None)  # è·³è¿‡æ ‡é¢˜è¡Œ
#                 count = sum(1 for _ in reader)
#                 total_length += count

#         episode = {
#             "episode_index": episode_index,
#             "tasks": [task_instr],
#             "length": total_length
#         }
#         lines.append(json.dumps(episode, ensure_ascii=False))
#         episode_index += 1

# os.makedirs(os.path.dirname(OUTPUT_FILE), exist_ok=True)
# # å†™å…¥ episodes.jsonl
# with open(OUTPUT_FILE, "w", encoding="utf-8") as f:
#     f.write("\n".join(lines))

# print(f"âœ… å®Œæˆï¼Œå·²ç”Ÿæˆ {OUTPUT_FILE}ï¼Œå…± {episode_index} ä¸ª episode")




#!/usr/bin/env python3
# -*- coding: utf-8 -*-
# ä».parquetè¯»å–step_length
"""
# ç”Ÿæˆ episodes.jsonlï¼š
# - æ¯ä¸ª .parquet æ–‡ä»¶å¯¹åº”ä¸€ä¸ª episode
# - ä»å¯¹åº”çš„ instruction.txt ä¸­è¯»å–ä»»åŠ¡æè¿°
# - length = parquet æ–‡ä»¶çš„è¡Œæ•°
"""

import os
import json
from pathlib import Path
import argparse
import pandas as pd

# ---------------- å‚æ•°è§£æ ----------------
parser = argparse.ArgumentParser(description='æ ¹æ® .parquet æ–‡ä»¶ç”Ÿæˆ episodes.jsonl')
parser.add_argument('--output_root', required=True, help='å­˜æ”¾ .parquet æ–‡ä»¶çš„æ ¹ç›®å½• (é€šå¸¸æ˜¯åŒ…å« chunk-* çš„ data ç›®å½•)')
parser.add_argument('--reorg_root', required=True, help='åŒ…å« instruction.txt çš„åŸå§‹é‡ç»„æ•°æ®ç›®å½•')
parser.add_argument('--output_file', required=True, help='è¾“å‡º episodes.jsonl æ–‡ä»¶è·¯å¾„')
args = parser.parse_args()

OUTPUT_ROOT = Path(args.output_root)
REORG_ROOT = Path(args.reorg_root)
OUTPUT_FILE = Path(args.output_file)

os.makedirs(OUTPUT_FILE.parent, exist_ok=True)

episode_index = 0
lines = []

# ---------------- æœç´¢ .parquet æ–‡ä»¶ ----------------
chunk_folders = sorted([p for p in OUTPUT_ROOT.iterdir() if p.is_dir() and p.name.startswith("chunk-")])
if not chunk_folders:
    raise FileNotFoundError(f"æœªæ‰¾åˆ°ä»»ä½• chunk-* ç›®å½•ï¼Œè¯·æ£€æŸ¥è·¯å¾„ï¼š{OUTPUT_ROOT}")

for chunk_folder in chunk_folders:
    parquet_files = sorted(chunk_folder.glob("episode_*.parquet"))
    if not parquet_files:
        print(f"[WARN] {chunk_folder} ä¸­æœªæ‰¾åˆ° parquet æ–‡ä»¶")
        continue

    for pq_file in parquet_files:
        try:
            # è¯»å– parquet æ–‡ä»¶ï¼Œç»Ÿè®¡ step æ•°
            df = pd.read_parquet(pq_file)
            length = len(df)

            # ä»æ–‡ä»¶åæˆ–ç›®å½•æ¨æ–­ä»»åŠ¡ç´¢å¼•ï¼ˆæ ¹æ®ä½ çš„ç»“æ„ï¼‰
            # å‡è®¾ chunk-000 å¯¹åº” reorg_root ä¸‹çš„ 0ï¼Œchunk-001 å¯¹åº” 1 ...
            type_idx = int(chunk_folder.name.split('-')[-1])+1
            q_dir = REORG_ROOT / str(type_idx)
            instr_file = q_dir / "instruction.txt"

            if instr_file.exists():
                with open(instr_file, "r", encoding="utf-8") as f:
                    instruction = f.read().strip()
            else:
                instruction = f"[æœªæ‰¾åˆ°æŒ‡ä»¤: {instr_file}]"
                assert False

            episode = {
                "episode_index": episode_index,
                "tasks": [instruction],
                "length": length
            }

            lines.append(json.dumps(episode, ensure_ascii=False))
            print(f"âœ… {pq_file.name} â†’ episode_index={episode_index}, step={length}")
            episode_index += 1

        except Exception as e:
            print(f"âŒ å¤„ç†å¤±è´¥: {pq_file}, é”™è¯¯: {e}")

# ---------------- å†™å‡º JSONL ----------------
with open(OUTPUT_FILE, "w", encoding="utf-8") as f:
    f.write("\n".join(lines))

print(f"\nğŸ¯ å·²ç”Ÿæˆ {OUTPUT_FILE}ï¼Œå…± {episode_index} ä¸ª episode")
